import json
import torch
import argparse

from data_selection.similarity import SiameseSimilarity
from data_selection.influence import compute_influence, best_influences
from train import train
from test import test
from utils import get_trainers, plot_model_loss

DEFAULT_BATCH_SIZE = 16
DEFAULT_EPOCHS = 40
DEFAULT_LEARNING_RATE = 1e-5
DEFAULT_RANK = 64

def parse_args():
    parser = argparse.ArgumentParser(
        description="Script for training and evaluating molecular models with fine-tuning options."
    )

    parser.add_argument(
        "-m", "--mode", choices=["train", "test"], default=None,
        help="Mode of operation: 'None', 'train' or 'test'. If None, then data selection methods are considered. Default: 'None'."
    )
    
    parser.add_argument(
        "-d", "--data-selection", choices=["influence", "similarity"], default=None,
        help="Data selection method: 'influence' or 'similarity'. Default: 'None'."
    )

    parser.add_argument(
        "-t", "--model-type", choices=["regression", "mlm", "mlm_regression"], default="regression",
        help="Model type: 'regression', 'mlm', or 'mlm_regression'. Default: 'mlm_regression'."
    )

    parser.add_argument(
        "-b", "--batch-size", type=int, default=DEFAULT_BATCH_SIZE,
        help=f"Batch Size. Default: {DEFAULT_BATCH_SIZE}."
    )

    parser.add_argument(
        "-e", "--epochs", type=int, default=DEFAULT_EPOCHS,
        help=f"Number of training epochs. Default: {DEFAULT_EPOCHS}."
    )

    parser.add_argument(
        "-l", "--learning-rate", type=float, default=DEFAULT_LEARNING_RATE,
        help=f"Learning rate. Default: {DEFAULT_LEARNING_RATE}."
    )

    parser.add_argument(
        "-s", "--save-weights-type", choices=["best", "last"], default="best",
        help="Weight saving strategy: 'best' or 'last'. Default: 'best'."
    )

    parser.add_argument(
        "-w", "--weights", type=str, default=None,
        help="Path to model weights. Required for training 'mlm_regression' (pre-trained MLM) or external data selection in 'influence'/'similarity' modes."
    )

    parser.add_argument(
        "-p", "--peft-type", choices=["biffit", "lora", "ia3"], default=None,
        help="Type of Parameter-Efficient Fine-Tuning (PEFT). If freezing is applied, this overrides it. Options: 'biffit', 'lora', 'ia3'. Default: None."
    )

    parser.add_argument(
        "-r", "--rank", type=int, default=DEFAULT_RANK,
        help=f"LoRA rank value (only used if --peft-type is 'lora'). Default: {DEFAULT_RANK}."
    )

    parser.add_argument(
        "-f", "--freeze-layers", choices=["partial", "full"], default=None,
        help="Freezing the last three attention layers (partial) or all layers in the model. Options: 'partial', 'full'. Default: None"
    )

    parser.add_argument(
        "-x", "--external-data", default=None,
        help="Selected external dataset path generated by data selection methods. If included, then training data will be combined with external data."
    )

    args = parser.parse_args()


    # Validate weight requirement based on mode
    if args.mode in ["test", "influence", "similarity"] and args.weights is None:
        parser.error(f"The --weights argument is required for mode '{args.mode}'.")

    return args

def main():
    args = parse_args()
    (
        model, 
        train_dataloader, 
        val_dataloader, 
        test_dataloader, 
        train_dataset,
        val_dataset,
        test_dataset,
        external_df,
        external_dataset, 
        criterion, 
        optimizer, 
        scheduler, 
        device
    ) = get_trainers(
        args.mode, 
        args.data_selection, 
        args.model_type, 
        args.batch_size, 
        args.learning_rate, 
        args.external_data, 
        args.weights, 
        args.freeze_layers, 
        args.peft_type, 
        args.rank
    )

    model.to(device)
    save_best_weights = True if args.save_weights_type == "best" else False
    
    if args.mode is None:
        if args.data_selection == "influence":
            scores = compute_influence(model, criterion, external_dataset, test_dataset, device)
            best_influences(external_df, scores)
        elif args.data_selection == "similarity":
            s_func = SiameseSimilarity(model, val_dataset, external_dataset, external_df)
            s_func.calculate_similarity()

    elif args.mode == "train":
        print("[INFO]: Training starts.")
        stats = train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, args.epochs, device, args.model_type, save_best_weights=save_best_weights)

        if save_best_weights:
            model.load_state_dict(torch.load(f"./best_{args.model_type}_model.pt", weights_only=True))
        else:
            model.load_state_dict(torch.load(f"./last_{args.model_type}_model.pt", weights_only=True))
        stats["losses"]["test"] = test(model, test_dataloader, criterion, args.model_type, device)
        plot_model_loss(stats, args.model_type)    

        with open(f"{args.model_type}_results.json", 'w') as f:
            json.dump(stats, f, indent=4)

    elif args.mode == "test":
        print("[INFO]: Test starts.")
        test_result = test(model, test_dataloader, criterion, args.model_type, device)
        print(f"Test Loss: {test_result}")


if __name__ == '__main__':
    main()
